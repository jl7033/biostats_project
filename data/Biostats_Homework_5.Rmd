---
title: "HW 5"
author: "Wayne Monical wem2121"
date: "2024-12-09"
output: pdf_document
---

  
```{r}
library(datasets)
library(tidyverse)
library(modelr)
```

```{r}
state.x77 <- 
  as.data.frame(datasets::state.x77) %>% 
  janitor::clean_names()
```


## a)

Providing descriptive statistics for all variables of interest
```{r}
summary(state.x77)
```


## b)

Examinng plots

```{r}
state.x77 %>% 
  ggplot(aes(x = life_exp))+
  geom_histogram()
```


```{r}
state.x77 %>% 
  ggplot(aes(x = income))+
  geom_histogram()
```

```{r}
state.x77 %>% 
  ggplot(aes(x = murder, y = life_exp))+
  geom_point()
```


## c)

### Automatically Selecting a Model 

We use forward and backward selection to determine a linear model.
```{r}
full_model <- lm(life_exp ~ ., data = state.x77)

forward_model <- step(full_model, direction = "forward")
backward_model <- step(full_model, direction = "backward")
```


```{r}
summary(full_model)
```


```{r}
summary(forward_model)
```
```{r}
summary(backward_model)
```


### Discussion

The two methods of automatically selecting the model features, forward selection and backward selection by AIC, did not agree. The forward approach selected the all model features. The penalized method approach selected only population, murder, high school graduation rate, and frost.

The inclusion of the area parameter is a close call. With a t-value of only -0.044, it was the least significant of the predictors in the AIC model, and clearly should not have been included. The forward selection model did not do any selection. In this case, I would opt for the backward selection model.


### HS Graduation versus Illiteracy

The observations in the sample have a correlation of -0.6571 between the rates of illiteracy and high school graduation. Both models include high school graduation with a positive effect on life expectancy. The backwards model does not include the illiteracy rate. Because these two features are correlated, and high school graduation rate is a predictor of life expectancy, illiteracy must also be a predictor of life expectancy. However, since illiteracy was not selected in the backwards selection process, we may conclude that high school graduation is a better predictor of life expectancy and illiteracy provides no more significant information. 
```{r}
print(cor(state.x77$illiteracy, state.x77$hs_grad))

state.x77 %>% 
  ggplot(aes(x= illiteracy, y = hs_grad)) + 
  geom_point()
```

## d)

We will train a model with AIC and BIC selection criteria. 
```{r}
bic_model <- step(full_model, direction = "both", k = log(nrow(state.x77)))
aic_model <- step(full_model, direction = "both", k = 2)
```

```{r}
summary(bic_model)
```

## e)

Automatically selecting a model with Lasso. The function `cv.glmnet` does a k-fold cross validation of the model with a variety of lambda values. The lambda value resulting in the lowest error is given by the `lambda.min` variable. We use this value for lambda to train a fresh model, and print its coefficients below. 
```{r}
library(glmnet)
y = pull(state.x77, life_exp)
x = as.matrix(select(state.x77, -life_exp))
lasso_model <- cv.glmnet(x, y)
best_lambda = lasso_model$lambda.min
best_lasso = glmnet(x, y, family = 'gaussian', lambda = best_lambda)
coef(best_lasso)

```

## f) 

The three models under consideration have selected the same variables, namely population, murders, high school graduation, and frost. The AIC and BIC models, having no penalization, have selected the exact same coefficients. The LASSO model, having a penalty, has selected smaller coefficients, but they are all in the same direction. 

```{r}
summary(backward_model)
```



```{r}
summary(bic_model)
```

```{r}
coefficients(best_lasso)
```

### Cross Validaiton 

I will now evaluate the models with cross validation. Because the AIC and BIC models are the same, I will only test the AIC model. In the end, the average RMSE of the two models were very similar, both around 0.70. The LASSO model had a varied performance, whereas the AIC model had consistent performance. With this in mind, I would choose the AIC model because it is unbiased, and because it had a more consistent RMSE under cross validation. 

```{r}
create_lasso_model = function(data){
  y = pull(as.data.frame(data), life_exp)
  x = as.matrix(select(as.data.frame(data), -life_exp))
  return(glmnet(x, y, family = 'gaussian', lambda = best_lambda))
}

get_lasso_preds = function(model, newx){
  newx = as.matrix(select(as.data.frame(newx), -life_exp))
  return(predict(model, newx))
}

difference = function(x,y){
  #print(as.vector(x))
  #print(pull(as.data.frame(y), life_exp))
  
  return(sum((as.vector(x) - pull(as.data.frame(y), life_exp))^2) / 5)
  
}

get_lasso_rmse = function(model, data){
  
}

aic_formula = 'life_exp ~ population + murder + hs_grad + frost'

cv_folds <- crossv_kfold(state.x77, k = 10)

cv_folds = 
  cv_folds %>% 
  mutate(
    aic_model = map(train, \(x) lm(formula = aic_formula,x))
    , aic_rmse = map2_dbl(aic_model, test, \(x, y) modelr::rmse(x, y))
    , lasso_model = map(train, \(x) create_lasso_model(x))
    # , test_df = map(test, as.data.frame)
    , lasso_pred = 
        map2(lasso_model, test, \(x, y) get_lasso_preds(x, y))
    , lasso_rmse = 
        map2(lasso_pred, test, difference)
  ) %>% 
  select(aic_rmse, lasso_rmse) %>% 
  unnest(aic_rmse, lasso_rmse) %>% 
  pivot_longer(
    cols = c('aic_rmse', 'lasso_rmse')
    , names_to = 'model'
    , values_to = 'rmse'
  )
```


```{r}
cv_folds %>% 
  ggplot(aes(x = model, y = rmse))+
  geom_violin()+
  labs(title = 'AIC Versus LASSO RMSE')
```


```{r}
cv_folds %>% 
  group_by(model) %>% 
  summarize(average_rmse = mean(rmse)) %>% 
  knitr::kable()
```

### AIC Model Diagnostics

Plotting the residuals against the fitted values, we see no significant relationship.
```{r}
plot(predict(aic_model, state.x77), aic_model$residuals)
```

Examining the residuals for normality, we see some deviation, but not egregious deviation. 
```{r}
qqnorm(aic_model$residuals)
```

Examining the residuals versus leverage, we find that no point is exerting fully undue leverage. The worst offender is Hawaii, but the ratio of standardized residuals to leverage is less than 0.5, so it is still acceptable. 

```{r}
plot(aic_model, which = 5)
```


## g) 

Population, murder rate, high school graduation rate, and frost were all shown to have an effect on life expectancy. To get the most accurate model values possible, we recommend that the researcher use the model selected with AIC. Murder has the strongest effect on life expectancy. It is a negative effect. This may be because murders can happen at any age, and therefore will decrease life expectancy. There may be another effect that the murder rate is associated with poverty, which also may decrease life expectancy. High school graduation rate was found to have the strongest positive relationship with life expectancy. State population and frost were also found to be relevant, but weaker factors in life expectancy.

